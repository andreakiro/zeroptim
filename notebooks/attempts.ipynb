{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grad-corr and dir-sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective\n",
    "- compute gradient correlation and\n",
    "- compute directional sharpness\n",
    "- as presented in https://arxiv.org/pdf/2306.00204.pdf\n",
    "\n",
    "$$f(x_{t+1}) = f(x_t) + \\nabla f(x_t) ^T (x_{t+1} - x_t) + \\frac{1}{2} (x_{t+1}-x_t) \\nabla^2 f(x_t) (x_{t+1}-x_t) + O( \\eta ^3)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load utils to go ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeroptim.mlp import MLP\n",
    "from zeroptim.data import loader\n",
    "from zeroptim.utils import parse_yaml_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.3087, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = loader('mnist-digits')\n",
    "m = MLP(**parse_yaml_config('mlp.yaml'))\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "m.norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (act_func): ReLU()\n",
       "  (out_func): Softmax(dim=1)\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform a single opt step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3130, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3015, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# fix (inputs, targets) for a batch\n",
    "inputs, targets = next(iter(dataloader))\n",
    "inputs = inputs.flatten(start_dim=1)\n",
    "\n",
    "# store a clone of model m at time t\n",
    "m_ = m.clone()\n",
    "\n",
    "# optimization step on original model m\n",
    "opt.zero_grad()\n",
    "loss = crit(m(inputs), targets)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "# compute the parameters update step\n",
    "update_step = [p.detach() - p_.detach() for p, p_ in zip(m.parameters(), m_.parameters())]\n",
    "\n",
    "# verify there was a change in model performances\n",
    "print(crit(m_(inputs), targets))\n",
    "print(crit(m(inputs), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "print(len(update_step)) # one update-step entry per weights&biases\n",
    "print(update_step[0].shape) # verify that shape of layer1 is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to compute grad-corr using `jvp`\n",
    "- the thing is that `jvp` was intended to compute gradients wrt. inputs\n",
    "- not wrt. the model parameters; so we need to hack it a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed tentative\n",
    "- didnt understand properly how jvp computed Jacobian at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say we want to compute jvp for first layer only\n",
    "p_ = next(m_.parameters()).detach()\n",
    "p = next(m.parameters()).detach()\n",
    "ps = p - p_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3130), tensor(0.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jvp(\n",
    "    func = lambda p: crit(m_(inputs), targets), # previous timestep model\n",
    "    inputs = p_, # previous timestep parameters (dummy input to trick jvp)\n",
    "    v = ps, # parameters update step, the direction where we stepped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the jvp result is null. this make sense as \n",
    "# jvp perturbs func() with small steps in inputs in direction of v\n",
    "# and because our func is a constant, then jvp is null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second tentative\n",
    "- bit hacky and pretty ugly but should work in theory\n",
    "- don't understand why it does not lead to some jvp value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "# verify both things point to the same weight matrix\n",
    "print(m.state_dict()['layers.0.0.weight'].shape)\n",
    "print(next(m.parameters()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(params: torch.Tensor, wnb_id='layers.0.0.weight'):\n",
    "    model = m_.clone() # clone original model (timestep-1)\n",
    "    sd = model.state_dict() # get params dict of that model\n",
    "    sd[wnb_id] = params # hack and update a specific layer\n",
    "    model.load_state_dict(sd) # load updated hacked state dict\n",
    "    return crit(model(inputs), targets) # loss wrt. new params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3130), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jvp(\n",
    "    func = func, # hacky function to compute jvp for a layer at least\n",
    "    inputs = next(m_.parameters()).detach(), # 'layers.0.0.weight'\n",
    "    v = ps, # parameters update step (direction)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really confused about why it's not working either\n",
    "# jvp should be able to perturb func() with params in direction of v\n",
    "# and return the jvp of func wrt. params in direction of v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyway this is nothing efficient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just do it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad-corr: tensor(-0.0190)\n",
      "tensor(2.2829, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2639, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# fix (inputs, targets) for a batch\n",
    "inputs, targets = next(iter(dataloader))\n",
    "inputs = inputs.flatten(start_dim=1)\n",
    "\n",
    "# store a clone of model m at time t\n",
    "# could also only store p_ = [p.detach() for p in m.parameters()]\n",
    "m_ = m.clone()\n",
    "\n",
    "# optimization step on original model m\n",
    "opt.zero_grad()\n",
    "loss = crit(m(inputs), targets)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "# parameter update step or direction\n",
    "vs = [\n",
    "    p.detach() - p_.detach() \n",
    "    for p, p_ in zip(m.parameters(), m_.parameters())\n",
    "]\n",
    "\n",
    "# compute jvp manually;\n",
    "# sum the product of grad_i * update_step_i for each parameter i\n",
    "# WARNING: need to make sure p.grad still exist at this point\n",
    "gcorr = sum(\n",
    "    (p.grad * v).sum() \n",
    "    for p, v in zip(m.parameters(), vs)\n",
    ")\n",
    "\n",
    "# print final results\n",
    "print(\"grad-corr:\", gcorr)\n",
    "\n",
    "# verify there was a change in model performances\n",
    "print(crit(m_(inputs), targets))\n",
    "print(crit(m(inputs), targets)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same for directional sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad-corr: tensor(-0.0133, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# fix (inputs, targets) for a batch\n",
    "inputs, targets = next(iter(dataloader))\n",
    "inputs = inputs.flatten(start_dim=1)\n",
    "\n",
    "# store previous parameters\n",
    "p_ = [p.detach().clone() for p in m.parameters()]\n",
    "\n",
    "# compute first order gradients\n",
    "opt.zero_grad()\n",
    "loss = crit(m(inputs), targets)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "# store gradients and update-directions\n",
    "gs = [p.grad for p in m.parameters()]\n",
    "vs = [p2 - p1 for p2, p1 in zip(m.parameters(), p_)]\n",
    "\n",
    "# compute gradient correlations (grad * Dw)\n",
    "gcorr = sum((g * v).sum()for g, v in zip(gs, vs))\n",
    "\n",
    "# compute directional sharpness (Dw * H * Dw)\n",
    "gs = [g.requires_grad_(True) for g in gs]\n",
    "vs = [v.requires_grad_(True) for v in vs]\n",
    "p_ = [p.requires_grad_(True) for p in p_]\n",
    "# vHv = \n",
    "\n",
    "# print final results\n",
    "print(\"grad-corr:\", gcorr)\n",
    "# print(\"dir-sharp:\", vHv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the same problem when using torch.autograd.functional.vhp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir-sharp: tensor(0., grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def func(*params):\n",
    "    # re-assing parameters to model\n",
    "    with torch.no_grad():\n",
    "        for p, new_p in zip(m.parameters(), params):\n",
    "            p.copy_(new_p.data)\n",
    "    return crit(m(inputs), targets)\n",
    "\n",
    "# compute the vector-Hessian-product\n",
    "_, vhp = torch.autograd.functional.vhp(func, tuple(m.parameters()), tuple(v for v in vs))\n",
    "\n",
    "# flatten vhp and compute the dot product with flattened v\n",
    "v_flat = torch.cat([v.flatten() for v in vs])\n",
    "vhp_flat = torch.cat([vhpi.flatten() for vhpi in vhp])\n",
    "vHv = torch.dot(vhp_flat, v_flat)\n",
    "\n",
    "# print the result\n",
    "print(\"dir-sharp:\", vHv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to compute manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None, None, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(gs, p_, vs, allow_unused=True) # hum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mezo-qrlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load a standard mlp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zeroptim.mlp import MLP\n",
    "from zeroptim.data import loader\n",
    "from zeroptim.utils import parse_yaml_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.3087, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = loader('mnist-digits')\n",
    "m = MLP(**parse_yaml_config('mlp.yaml'))\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "crit = torch.nn.CrossEntropyLoss()\n",
    "m.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take one gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix (inputs, targets) for a batch\n",
    "inputs, targets = next(iter(dataloader))\n",
    "inputs = inputs.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store parameters prior the update\n",
    "prev_params = tuple([p.clone() for p in m.parameters()])\n",
    "\n",
    "# perform one optimization step\n",
    "opt.zero_grad()\n",
    "loss = crit(m(inputs), targets)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "# store params post-update, gradients, update-directions\n",
    "cur_params = tuple([p.clone() for p in m.parameters()])\n",
    "gs = [p.grad.clone() for p in m.parameters()]\n",
    "vs = [p2.detach() - p1.detach() for p2, p1 in zip(cur_params, prev_params)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute gradient correlation manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.312971591949463\n",
      "Gradient correlation: -0.011501125991344452\n"
     ]
    }
   ],
   "source": [
    "gcorr = sum((g * v).sum()for g, v in zip(gs, vs))\n",
    "print(f'Forward value: {loss}')\n",
    "print(f'Gradient correlation: {gcorr.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working attempt using torch.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/combining-functional-jvp-with-a-nn-module/81215/2\n",
    "\n",
    "def del_attr(obj, names):\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])\n",
    "\n",
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)\n",
    "\n",
    "def make_functional(model):\n",
    "    # Remove all the parameters in the model\n",
    "    orig_params = tuple(model.parameters())\n",
    "    names = []\n",
    "    for name, p in list(model.named_parameters()):\n",
    "        del_attr(model, name.split(\".\"))\n",
    "        names.append(name)\n",
    "    return orig_params, names\n",
    "\n",
    "def restore_functional(model, orig_params, names):\n",
    "    # Restore all the parameters in the model\n",
    "    for name, p in zip(names, orig_params):\n",
    "        set_attr(model, name.split(\".\"), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass before jvp 2.301527976989746\n"
     ]
    }
   ],
   "source": [
    "print(f'Forward pass before jvp {crit(m(inputs), targets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.312971591949463\n",
      "Gradient correlation (jvp): -0.011501124128699303\n",
      "Gradient correlation (manual): -0.011501125991344452\n"
     ]
    }
   ],
   "source": [
    "def func(*params):\n",
    "    for name, p in zip(names, params):\n",
    "        set_attr(m, name.split(\".\"), p)\n",
    "    return crit(m(inputs), targets)\n",
    "\n",
    "orig_params, names = make_functional(m)\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, prev_params, v=tuple(vs))\n",
    "restore_functional(m, orig_params, names)\n",
    "\n",
    "# print final results\n",
    "print(f'Forward value: {fwdvalue}')\n",
    "print(f'Gradient correlation (jvp): {jvp}')\n",
    "print(f'Gradient correlation (manual): {gcorr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass after jvp 2.301527976989746\n"
     ]
    }
   ],
   "source": [
    "print(f'Forward pass after jvp {crit(m(inputs), targets)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute vhv using torch.hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.312971591949463\n",
      "Directional sharpness (vhv): -0.0009699021466076374\n"
     ]
    }
   ],
   "source": [
    "orig_params, names = make_functional(m)\n",
    "fwdvalue, hvp = torch.autograd.functional.vhp(func, prev_params, v=tuple(vs))\n",
    "vhv = sum((v * hv).sum() for v, hv in zip(vs, hvp))\n",
    "restore_functional(m, orig_params, names)\n",
    "\n",
    "# print final results\n",
    "print(f'Forward value: {fwdvalue}')\n",
    "print(f'Directional sharpness (vhv): {vhv}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous attemps and failed work-arounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verify `jvp` functions on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.302\n",
      "JVP (on inputs): 0.027\n"
     ]
    }
   ],
   "source": [
    "def func(*inputs):\n",
    "    l = crit(m(inputs[0]), targets)\n",
    "    return l\n",
    "\n",
    "primals = (inputs, inputs,) # multidim to check broadcasting works\n",
    "tangents = (torch.randn(inputs.shape), torch.randn(inputs.shape),)\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, primals, tangents)\n",
    "\n",
    "print(f'Forward value: {round(fwdvalue.item(), 3)}')\n",
    "print(f'JVP (on inputs): {round(jvp.item(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute gradient correlation using `jvp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 01: reassign model in the closure (dependency on params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.313\n",
      "JVP (on params): 0.0\n"
     ]
    }
   ],
   "source": [
    "def reassign(*params):\n",
    "    with torch.no_grad():\n",
    "        # re-assign parameters of a model in-place\n",
    "        for p, new_p in zip(m.parameters(), params):\n",
    "            p.copy_(new_p)\n",
    "    m.zero_grad() # zero-out gradients\n",
    "\n",
    "def func(*params):\n",
    "    reassign(*params) # get back to prev params\n",
    "    l = crit(m(inputs), targets) # compute loss\n",
    "    reassign(*cur_params) # get back to new params\n",
    "    return l\n",
    "\n",
    "# compute gradient correlations with jvp\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, prev_params, tuple(vs))\n",
    "\n",
    "print(f'Forward value: {round(fwdvalue.item(), 3)}')\n",
    "print(f'JVP (on params): {round(jvp.item(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 02: recreate model from scratch in closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.313\n",
      "JVP (on params): 0.0\n"
     ]
    }
   ],
   "source": [
    "def func(*params):\n",
    "    m = MLP(**parse_yaml_config('mlp.yaml'))\n",
    "    with torch.no_grad():\n",
    "        for p, new_p in zip(m.parameters(), params):\n",
    "            p[:] = new_p\n",
    "    return crit(m(inputs), targets)\n",
    "\n",
    "# compute gradient correlations with jvp\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, prev_params, tuple(vs))\n",
    "\n",
    "print(f'Forward value: {round(fwdvalue.item(), 3)}')\n",
    "print(f'JVP (on params): {round(jvp.item(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 03: use a param perturbation to avoid copying params and detaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.313\n",
      "JVP (on params): 0.0\n"
     ]
    }
   ],
   "source": [
    "def func(*params):\n",
    "    # create a perturbation in model parameters\n",
    "    perturb = [prev - cur for prev, cur in zip(params, cur_params)]\n",
    "    \n",
    "    # apply the perturbation to the model to rebase model at *params\n",
    "    for p, eps in zip(m.parameters(), perturb):\n",
    "        p.data += eps\n",
    "\n",
    "    return crit(m(inputs), targets)\n",
    "\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, prev_params, tuple(vs))\n",
    "\n",
    "# print final results\n",
    "print(f'Forward value: {round(fwdvalue.item(), 3)}')\n",
    "print(f'JVP (on params): {round(jvp.item(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt 04: write the forward pass manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward value: 2.72\n",
      "JVP (on params): -0.067\n"
     ]
    }
   ],
   "source": [
    "def func(*params):\n",
    "\n",
    "    w1, b1 = params[0], params[1]\n",
    "    w2, b2 = params[2], params[3]\n",
    "    w3, b3 = params[4], params[5]\n",
    "\n",
    "    z1 = m.act_func(inputs @ w1.T + b1)\n",
    "    z2 = m.act_func(z1 @ w2.T + b2)\n",
    "    z3 = z2 @ w3.T + b3\n",
    "\n",
    "    return crit(z3, targets)\n",
    "\n",
    "fwdvalue, jvp = torch.autograd.functional.jvp(func, prev_params, tuple(vs))\n",
    "\n",
    "# print final results\n",
    "print(f'Forward value: {round(fwdvalue.item(), 3)}')\n",
    "print(f'JVP (on params): {round(jvp.item(), 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-mezo-qrlr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
